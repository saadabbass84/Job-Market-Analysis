{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4 : Job Market Analysis\n",
    "\n",
    "### Notebook 01: Data Gathering (Web Scrape)\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "- [1.0 Introduction](#Intro)\n",
    "- [2.0 Web Scrape](#scrapemain)\n",
    "    - [2.1 Defining scraping functions](#functions)\n",
    "    - [2.2 Scraping the web](#scrape)\n",
    "- [3.0 Next Steps](#next)\n",
    "\n",
    "## 1.0 Introduction <a name=\"Intro\"></a>\n",
    "The goal of this project is to answer the following questions:\n",
    "1. Which factors in the job market have the most affect on salary?\n",
    "2. Is it possible to identify the key skills and buzzwords across job category / title?\n",
    "\n",
    "In order to answer the above questions, I will gather data from the job search site [SEEK Limited AU](https://www.seek.com.au/). For this project I will limit the study to available jobs in data related fields (e.g. data science, data analyst etc) in the Sydney and Melbourne areas.\n",
    "\n",
    "I will be splitting this project in 3 notebooks as follows:\n",
    "- **Notebook 01: Data Gathering (Web Scrape)**\n",
    "- Notebook 02: Data Cleaning and Exporatory Analysis\n",
    "- Notebook 03: Predictive Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from scrapy.selector import Selector\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Web Scrape <a name=\"scrapemain\"></a>\n",
    "The following code will scrape the data from the job site SEEK.com.au. I have defined two functions as follows:\n",
    "- The first function will scrape the search result page and identify all the href links to the job advertisement pages.\n",
    "- The second function will go to each job advertisement page, and scrape the available data.\n",
    "\n",
    "The output of these functions will give me a dataframe with all the scraped information which I will use for analysis and modelling. For the scrape and parsing, I am using Requests for fetching the website, and a combination of BeautifulSoup and Scrapy/Xpath for the parsing process.\n",
    "\n",
    "**Important Note:**\n",
    "> The code below is functional at the time of creating this notebook. However, it is possible in future that the original webpage structure may change, and would not give the intended results!\n",
    "\n",
    "### 2.1 Defining scraping functions <a name=\"functions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scraping function for each job page\n",
    "def individual_page(url):\n",
    "    result = requests.get(url)\n",
    "    \n",
    "    if result.status_code == 200:\n",
    "        x_select = Selector(text=result.text)\n",
    "        job_title = x_select.xpath('//*/*[@*=\"job-detail-title\"]/span/h1/text()').extract()[0] # Extract Title\n",
    "        advertiser = x_select.xpath('//*/h2/span/span/text()').extract()                       # Extract Advertiser\n",
    "        if len(advertiser) > 0:\n",
    "            advertiser = advertiser[0]\n",
    "        else:\n",
    "            advertiser = None\n",
    "        rating = x_select.xpath('//*/h2/span/span/span/span/text()').extract()                 # Extract Advertiser Rating\n",
    "        if len(rating) > 0:\n",
    "            rating = rating[0]\n",
    "        else:\n",
    "            rating = None\n",
    "            \n",
    "        posted_date = x_select.xpath('//*/*[@*=\"job-detail-date\"]/span/span/text()').extract()[0]  # Extract Post Date\n",
    "        salary = x_select.xpath('//*/*[@*=\"jobInfoHeader\"]/dl/div/dd/span/span/text()').extract()  # Extract Salary\n",
    "        if len(salary) > 0:\n",
    "            salary = salary[0]\n",
    "        else:\n",
    "            salary = None\n",
    "            \n",
    "        work_type = x_select.xpath('//*/*[@*=\"job-detail-work-type\"]/span/span/text()').extract()[0]  # Extract Contract Type\n",
    "        category = x_select.xpath('//*/section[@*=\"jobInfoHeader\"]/dl/div/dd/span/span/strong/text()').extract()[0]  # Job Category\n",
    "        subcategory = x_select.xpath('//*/section[@*=\"jobInfoHeader\"]/dl/div/dd/span/span/span/text()').extract()[0] # Job Category\n",
    "\n",
    "        # Extract Job description text\n",
    "        results_parsed = BeautifulSoup(result.text, 'lxml')\n",
    "              \n",
    "        body = results_parsed.find('div', {'data-automation': 'mobileTemplate'}).text\n",
    "        \n",
    "        return job_title, advertiser, rating, posted_date, salary, work_type, category, subcategory, body\n",
    "    \n",
    "    else:\n",
    "        print('Failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for job search page scraping\n",
    "def job_scrape(job, location, res_per_page=20):\n",
    "\n",
    "    scraped = {'searched_job' : [],\n",
    "               'searched_city' : [],\n",
    "               'job_title' : [],\n",
    "               'advertiser' : [],\n",
    "               'advertiser_rating' : [],\n",
    "               'date_posted' : [],\n",
    "               'salary' : [],\n",
    "               'work_type' : [],\n",
    "               'category' : [],\n",
    "               'subcategory' : [],\n",
    "               'job_description' : [],\n",
    "               'url' : []\n",
    "              }\n",
    "\n",
    "    url = 'https://www.seek.com.au/' + job + '-jobs/in-' + location\n",
    "    result = requests.get(url)\n",
    "    x_selector = Selector(text=result.text)\n",
    "    num_results = np.ceil(int(x_selector.xpath('//*/*[@*=\"totalJobsCount\"]/text()').extract()[0].replace(',','')) / 20)\n",
    "    num_results = int(num_results)\n",
    "    \n",
    "    for i in range(1, num_results+1):\n",
    "        url = 'https://www.seek.com.au/' + job + '-jobs/in-' + location + '?page=' + str(i)\n",
    "        searches = requests.get(url)\n",
    "        x_selector = Selector(text=searches.text)\n",
    "        h_refs = x_selector.xpath('//*/*[@*=\"searchResults\"]/div/div/div/article/span/span/h1/a/@href').extract()\n",
    "        \n",
    "        href_count = 1\n",
    "        for h_ref in h_refs:\n",
    "            url = 'https://www.seek.com.au' + h_ref\n",
    "            print(url)\n",
    "            job_title, advertiser, rating, posted_date, salary, work_type, category, subcategory, body = individual_page(url)\n",
    "            scraped['searched_job'].append(job)\n",
    "            scraped['searched_city'].append(location)\n",
    "            scraped['job_title'].append(job_title)\n",
    "            scraped['advertiser'].append(advertiser)\n",
    "            scraped['advertiser_rating'].append(rating)\n",
    "            scraped['date_posted'].append(posted_date)\n",
    "            scraped['salary'].append(salary)\n",
    "            scraped['work_type'].append(work_type)\n",
    "            scraped['category'].append(category)\n",
    "            scraped['subcategory'].append(subcategory)\n",
    "            scraped['job_description'].append(body)\n",
    "            scraped['url'].append(url)\n",
    "            \n",
    "            href_count += 1\n",
    "            sleep(1)\n",
    "            \n",
    "    return pd.DataFrame(scraped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Scraping the web <a name=\"scrape\"></a>\n",
    "Now that the functions are defined, let us run the functions and get the data.\n",
    "\n",
    "\n",
    "**WARNING: Takes a very long time, do not run unless necessary!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: Takes a very long time, do not run unless necessary!!\n",
    "\n",
    "# Searching for Data Science and Data Analyst jobs in Sydney and Melbourne\n",
    "jobs = pd.DataFrame({'searched_job' : [],\n",
    "               'searched_city' : [],\n",
    "               'job_title' : [],\n",
    "               'advertiser' : [],\n",
    "               'advertiser_rating' : [],\n",
    "               'date_posted' : [],\n",
    "               'salary' : [],\n",
    "               'work_type' : [],\n",
    "               'category' : [],\n",
    "               'subcategory' : [],\n",
    "               'job_description' : [],\n",
    "               'url' : []\n",
    "              })\n",
    "\n",
    "for loc in ['All-Sydney-NSW', 'All-Melbourne-VIC']:\n",
    "    for job in ['data-scientist', 'data-analyst']\n",
    "    these_jobs = job_scrape(job, loc)\n",
    "    jobs = pd.concat([jobs, these_jobs], axis=0)\n",
    "    \n",
    "# Saving original scraped data to csv file:\n",
    "jobs.to_csv('./datasets/seekjobs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code will only scrape partially and append to the original data. I have run this multiple times to try and balance out the unbalanced searches across different search keywords (e.g. Data Analyst gives 4,000 results while Data Scientist only gives around a 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving subsequent Data Science jobs appended to original file\n",
    "jobs = pd.DataFrame({'searched_job' : [],\n",
    "               'searched_city' : [],\n",
    "               'job_title' : [],\n",
    "               'advertiser' : [],\n",
    "               'advertiser_rating' : [],\n",
    "               'date_posted' : [],\n",
    "               'salary' : [],\n",
    "               'work_type' : [],\n",
    "               'category' : [],\n",
    "               'subcategory' : [],\n",
    "               'job_description' : [],\n",
    "               'url' : []\n",
    "              })\n",
    "\n",
    "for loc in ['All-Sydney-NSW', 'All-Melbourne-VIC']:             # , 'All-Sydney-NSW', 'All-Melbourne-VIC'\n",
    "    for job in ['data-engineer']:                              # , 'data-analyst', 'data-scientist'\n",
    "        these_jobs = job_scrape(job, loc)\n",
    "    jobs = pd.concat([jobs, these_jobs], axis=0)\n",
    "\n",
    "cols_for_dupes = ['searched_city', 'job_title', 'advertiser', 'advertiser_rating', \n",
    "                  'salary', 'work_type', 'job_description']\n",
    "jobs = jobs.drop_duplicates(subset=cols_for_dupes, keep='first')    \n",
    "\n",
    "jobs.to_csv('./datasets/seekjobs.csv', header=False, mode='a')\n",
    "print('Number of jobs :', jobs.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Next Steps <a name=\"next\"></a>\n",
    "We have our data! In the next notebook (Notebook 02) I will go over the cleaning and exploratory analysis process.\n",
    "\n",
    "__________________________________________________________________\n",
    "**-- NOTEBOOK 01 END --**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
